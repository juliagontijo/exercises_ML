# Self-study
Tracking improvement on self-study about LLM / ML / NLP.

Goals: 
- understand Large Language Models Large Language Models (LLMs) architecture and optimization techniques
- apply what I've learned to projects with SOTA methods

Topics to learn:
- Data prep for training
- Base Models - pre training
- Fine tuning
- Perplexity
- Transformers
- Tokenizers
- Fine Tuning
- AI Agents

## [Exercises](https://github.com/juliagontijo/self_study/Exercises)
Solved exercises, proposed by online videos/tutorials/courses to check content understanding by the end of the lecture.
Go to [README](https://github.com/juliagontijo/self_study/Exercises/README.md) for description and linked resources used in each exercise.

## [Projects](https://github.com/juliagontijo/self_study/tree/main/Projects)
Projects to practice and implement different levels of LLM pipeline
Go to [README](https://github.com/juliagontijo/self_study/Projects/README.md) for description and linked resources used in each project.

## Overall resources
- [Lighning.ai by Sebastian Raschka](https://lightning.ai/sebastian)
- [HuggingFace](https://huggingface.co/)
- [philschmid Github repo](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/instruction-tune-llama-2-int4.ipynb)
